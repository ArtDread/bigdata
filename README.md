# bigdata-docker-compose

[Source code](https://github.com/vladErmakov07/bigdata-docker-compose) by [vladErmakov07](https://github.com/vladErmakov07)

## Setup

(Почти) настроенный докер с последним hadoop и сопутствующими инструментами на борту

Образ при сборке выкачивает много данных (ставит хадупы\юпитеры\хайвы и т.д.). Это норма.
Лучше не запускаться при подключении к лимитному интернету.

Для запуска:

1. Поставить docker + docker-compose на локальную машину

Для запуска hadoop:

1. Сначала запускаем неймноду:

    ```dockerfile
    command: ["hdfs", "namenode", "-format", "-force"] 
    ```

2. Так запуститься надо только в первый раз (либо, после того, как вы снесли образ и примонтированный раздел)
3. После того, как контейнер отработал и завершился, запускаемся с командой command:

    ```dockerfile
    command: ["hdfs", "namenode"]
    ```

4. После неймноды поднимаем датаноды, нодменеджеры и т.д.

Для запуска hive:

1. Сначала поднимаем постгрес.

2. Затем поднимаем метастор:

    ```dockerfile
    command: ["schematool", "--dbType", "postgres", "--initSchema"]
    ```

3. Так запуститься надо только в первый раз (либо, после того, как вы снесли образ и примонтированный раздел)
4. После того, как контейнер отработал и завершился, запускаемся с командой command:

    ```dockerfile
    command: ["hive", "--service", "metastore"]
    ```

5. После метастора запускаем hiveserver2

## Homework 1

### Блок 1. Развертывание локального кластера Hadoop

1. Развернуть локальный кластер в конфигурации 1 NN, 1-3 DN (в зависимости от доступных мощностей ноутбука), 1-2 NM, 1 RM
2. Изучить настройки и состояние NM и RM в веб-интерфейсе. Сделать скриншоты NN и RM.
3. Развернуть юпитер. Создать ноутбук. Из него с помощью питон кода (pyarrow), либо с помощью команд “hadoop fs” загрузить любой файл с данными на hdfs.
4. В отдельных ячейках выполнить команду:

    ```bash
    hadoop fs -ls $file_path && hadoop fs -cat $file_path
    ```

5. Запустить команду:

    ```bash
    hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 15 1800
    ```

### Блок 2. Решение нескольких MapRed задач

Требуется написать несколько map-reduce задач.
Для написания использовать питон и (по желанию) фреймворк [mrjob](https://mrjob.readthedocs.io/en/latest/guides.html).

Задачи следует протестировать как локально, так и на кластере.

В качестве результатов к каждой задаче нужно добавить в ваш репозиторий (результаты каждой задачи в отдельной папке):

* Скриншоты запуска из RM.
* Ноутбук с кодом mapred, командами запуска и диаграммами (если требуются).
* Результаты запуска в виде файлов.

#### Описание датасета

Есть [датасет](https://www.kaggle.com/datasets/xvivancos/star-wars-movie-scripts/data) со скриптами первых трех эпизодов звездных войн:

Структура датасета: csv файл, состоящий из трех колонок: порядковый номер фразы, персонаж, текст.

Пример:

```text
"2" "THREEPIO" "We're doomed!"
```

#### Задача 1: Болтун - находка для шпиона империи

Требуется найти топ самых разговорчивых персонажей - посчитать количество реплик у каждого, и выбрать 20 с самым большим значением.
Результаты отсортировать по количеству реплик (по убыванию).
Предоставить результаты для каждого эпизода, и отдельно по всем трем.

#### Задача 2: Воодушевляющая речь

Требуется найти самую длинную фразу каждого персонажа. Результат должен содержать пары (имя персонажа: его самая длинная фраза), а так же должен быть обратно отсортирован по длине фразы.

Предоставить результаты для каждого эпизода, и отдельно по всем трем.

#### Задача 3: Кто о чем, а ситх об абсолюте

Очистка должна быть произведена в коде mapper-а, на reducer должны приходить уже готовые bigram-ы.

Результат данной части может отличаться в зависимости от качества предварительной очистки, это норма.

Результаты каждого запуска прочитать и вывести на диаграмму любым удобным способом.

Для обработки текста обязательно использовать библиотеку [nltk](https://www.nltk.org/).

## Homework 2

### Блок 1. Запуск Spark application

1. Запустить спарк-сессию (SparkSession) с мастером YARN, 2-мя экзекьюторами и именем приложения “{фамилия}_spark”- перед этим обязательно выйдите из savemode в hdfs (hdfs dfsadmin -safemode leave).
Приложить скрин YARN-а, где запущено приложение, приложить скрин UI приложения spark.

2. Прочитать таблицы ratings, tags в директории ml-latest-small; отобразить количество строчек и в том, и в другом датасете.
Приложить скрин spark-ui с выполненной job-ой. Написать, сколько было выполнено стейджей и тасок.

В качестве результата приложите ноутбук с названием hw_spark в папке notebooks и кодом созданной спарк-сессии, скринами, и прочитанными датасетами.

### Блок 2. Работа с данными в Spark

1. Посчитать количество уникальных фильмов и уникальных юзеров в таблице “ratings”.

2. Посчитать, сколько было поставлено оценок >= 4.0

3. Вывести топ 100 фильмов с самым высоким рейтингом.

4. Посчитать разницу во времени в секундах между временем тегирования пользователя данного фильма и временем, когда пользователь поставил оценку фильму. В качестве ответа выведете среднюю дельту по времени.

5. Посчитать среднюю оценку от каждого пользователя, в качестве ответа выведете среднее от всех усредненных оценок всех пользователей.

### Блок 3. UDF в Spark

1. Обучите модель предсказания оценок по тегам с помощью TfidfVectorizer и SGDRegressor из модуля scikit-learn - тут уже можно сконвертировать два датасета в pandas через .toPandas
    * сначала  TfidfVectorizer обучаете на колонке “tag”
    * получаете численные признаки transform-ом от tfidf на той же колонке “tag”
    * обучаете SGDRegressor на новых численных признаках от  TfidfVectorizer-а с лейблом “rating”

2. Напишите UDF, которая делает предсказание рейтинга по столбцу “tag”

    * сначала transform от TfidfVectorizer
    * затем predict от SGDRegressor на полученных признаках из 1 этапа

3. Примените UDF к spar-dataframe-у и убедитесь, что udf работает. Приложите скрин дага вычислений этой джобы в spark-ui.

4. Напишите, чему равен корень суммы квадратов разностей (RMSE) между предсказанным и истинным значением рейтинга (напишите это на pyspark-е). Напишите, сколько было выполнено стейджей и тасок в рамках джобы.
