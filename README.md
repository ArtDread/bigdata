# bigdata-docker-compose

[Source code](https://github.com/vladErmakov07/bigdata-docker-compose) by [vladErmakov07](https://github.com/vladErmakov07)

## Setup

(Почти) настроенный докер с последним hadoop и сопутствующими инструментами на борту

Образ при сборке выкачивает много данных (ставит хадупы\юпитеры\хайвы и т.д.). Это норма.
Лучше не запускаться при подключении к лимитному интернету.

Для запуска:

1. Поставить docker + docker-compose на локальную машину

Для запуска hadoop:

1. Сначала запускаем неймноду:

    ```dockerfile
    command: ["hdfs", "namenode", "-format", "-force"] 
    ```

2. Так запуститься надо только в первый раз (либо, после того, как вы снесли образ и примонтированный раздел)
3. После того, как контейнер отработал и завершился, запускаемся с командой command:

    ```dockerfile
    command: ["hdfs", "namenode"]
    ```

4. После неймноды поднимаем датаноды, нодменеджеры и т.д.

Для запуска hive:

1. Сначала поднимаем постгрес.

2. Затем поднимаем метастор:

    ```dockerfile
    command: ["schematool", "--dbType", "postgres", "--initSchema"]
    ```

3. Так запуститься надо только в первый раз (либо, после того, как вы снесли образ и примонтированный раздел)
4. После того, как контейнер отработал и завершился, запускаемся с командой command:

    ```dockerfile
    command: ["hive", "--service", "metastore"]
    ```

5. После метастора запускаем hiveserver2

## Homework 1

### Блок 1. Развертывание локального кластера Hadoop

1. Развернуть локальный кластер в конфигурации 1 NN, 1-3 DN (в зависимости от доступных мощностей ноутбука), 1-2 NM, 1 RM
2. Изучить настройки и состояние NM и RM в веб-интерфейсе. Сделать скриншоты NN и RM.
3. Развернуть юпитер. Создать ноутбук. Из него с помощью питон кода (pyarrow), либо с помощью команд “hadoop fs” загрузить любой файл с данными на hdfs.
4. В отдельных ячейках выполнить команду:

    ```bash
    hadoop fs -ls $file_path && hadoop fs -cat $file_path
    ```

5. Запустить команду:

    ```bash
    hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 15 1800
    ```

### Блок 2. Решение нескольких MapRed задач

Требуется написать несколько map-reduce задач.
Для написания использовать питон и (по желанию) фреймворк [mrjob](https://mrjob.readthedocs.io/en/latest/guides.html).

Задачи следует протестировать как локально, так и на кластере.

В качестве результатов к каждой задаче нужно добавить в ваш репозиторий (результаты каждой задачи в отдельной папке):

* Скриншоты запуска из RM.
* Ноутбук с кодом mapred, командами запуска и диаграммами (если требуются).
* Результаты запуска в виде файлов.

#### Описание датасета

Есть [датасет](https://www.kaggle.com/datasets/xvivancos/star-wars-movie-scripts/data) со скриптами первых трех эпизодов звездных войн:

Структура датасета: csv файл, состоящий из трех колонок: порядковый номер фразы, персонаж, текст.

Пример:

```text
"2" "THREEPIO" "We're doomed!"
```

#### Задача 1: Болтун - находка для шпиона империи

Требуется найти топ самых разговорчивых персонажей - посчитать количество реплик у каждого, и выбрать 20 с самым большим значением.
Результаты отсортировать по количеству реплик (по убыванию).
Предоставить результаты для каждого эпизода, и отдельно по всем трем.

#### Задача 2: Воодушевляющая речь

Требуется найти самую длинную фразу каждого персонажа. Результат должен содержать пары (имя персонажа: его самая длинная фраза), а так же должен быть обратно отсортирован по длине фразы.

Предоставить результаты для каждого эпизода, и отдельно по всем трем.
